{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from data.task_multi import multi\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class layer(nn.Module):\n",
    "    def __init__(self, num, sigma = 1,hidden_dim=20, input_dimp = 20):\n",
    "        super(self, layer).__init__()\n",
    "        self.num = num \n",
    "        self.sigma = sigma\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.center = nn.ParameterList([torch.zeros([input_dim]) for _ in range(num)])\n",
    "        self.transform = nn.ModuleList([nn.Linear(input_dim, hidden_dim) for _ in range(m)])\n",
    "    def forward(self, h):\n",
    "        #h -> b*p_num * hidden\n",
    "        #batch * p_num * hidden -> b * p_num * num\n",
    "        dis = []\n",
    "        for c in self.center:\n",
    "            distance = torch.sum((h - c.view(1,1,-1))**2, dim=-1)/(2.0 * self.sigma)#b*p_num\n",
    "            dis.append(distance)\n",
    "        dis = torch.stack(dis, dim = -1) #b*p_num*num\n",
    "        prob = F.softmax(-dis, dim = -1) #b * p * num\n",
    "        # b * p * num & b * p\n",
    "        all_hidden = []\n",
    "        for i in range(h.shape[1]):\n",
    "            hidden = h[:,i,:]\n",
    "            post = []\n",
    "            for linear in self.transform:\n",
    "                post_linear = linear(hidden)#b*after_linear_hidden\n",
    "                post.append(post_linear)\n",
    "            post = torch.stack(post, dim=1)#b * num * after_hidden\n",
    "            all_hidden.append(post)\n",
    "        all_hidden = torch.stack(all_hidden, dim=1)#b * p_num * num * after_hidden\n",
    "        all_hidden = torch.sum(prob.unsqueezeu(dim=-1) * all_hidden, dim = 1)# b * num * after_hidden\n",
    "        return all_hidden\n",
    "    \n",
    "class clustering(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self, clustering).__init__()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
